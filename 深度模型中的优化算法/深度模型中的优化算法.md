某猪场的笔试题问到了深度学习中的优化算法，由于准备不足扑街了，后来补了一下这方面的知识，整理如下：

#基本算法

##随机梯度下降（SGD）

![]()

实践中，一般会线性衰减学习率直到第$\tau$次迭代：

$$\epsilon_k = (1-\alpha)\epsilon_0 + \alpha\epsilon_{\tau}$$

其中，$\alpha=\frac{k}{\tau}$。在$\tau$布迭代之后，一般使$\epsilon$保持常数。

##动量（momentum）

![]()

如果动量算法总是观测到梯度$g$，那么它会在方向$-g$上不停加速，直到达到最终速度，其中步长大小为：

$$\frac{\epsilon||g||}{1-\alpha}$$

因此，$\alpha=0.9$对应着最大速度10倍于梯度下降算法。

## Nesterov动量

![]()

Nesterov 动量中，梯度计算在施加当前速度之后。因此，Nesterov 动量可以解释为往标准动量方法中添加了一个校正因子。

在凸批量梯度下有改进，但是在随机梯度下没有改进收敛率。

# 参数初始化策略

##Gaussian initialization

## Xavier initialization

##He initialization

## Batch Normlization

参数初始化部分可以参考知乎的这篇[文章](https://zhuanlan.zhihu.com/p/25110150)。

# 自适应学习率算法

## AdaGrad

![]()

在凸优化背景下，AdaGrad算法具有一些令人满意的理论性质。

## RMSProp

![]()

RMSProp 算法(Hinton, 2012) 修改AdaGrad 以在非凸设定下效果更好，改变梯度积累为指数加权的移动平均。

![]()

## Adam

![]()

Adam通常被认为对超参数的选择相当鲁棒。

以上截图出自“花书”——Goodfellow的《深度学习》。





